{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook dependencies\n",
    "import pyspark\n",
    "from pyspark.sql.functions import col, expr\n",
    "from pyspark.sql.functions import regexp_extract, regexp_replace\n",
    "\n",
    "# note: the pyspark avg and mean functions are aliases of eachother\n",
    "from pyspark.sql.functions import concat, sum, avg, min, max, count, mean, lit\n",
    "\n",
    "# note: the following import, imports all pyspark sql functions similar to above\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# schema structures\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# creating the spark instance\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "\n",
    "# pandas, numpy, and matplotlib imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# pydatasets\n",
    "from pydataset import data\n",
    "\n",
    "# tqdm loading bar library\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import time # to be used in loop iterations\n",
    "\n",
    "# disabling warnings\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 841704 entries, 0 to 841703\n",
      "Data columns (total 14 columns):\n",
      " #   Column                Non-Null Count   Dtype  \n",
      "---  ------                --------------   -----  \n",
      " 0   case_id               841704 non-null  int64  \n",
      " 1   case_opened_date      841704 non-null  object \n",
      " 2   case_closed_date      823594 non-null  object \n",
      " 3   SLA_due_date          841671 non-null  object \n",
      " 4   case_late             841704 non-null  object \n",
      " 5   num_days_late         841671 non-null  float64\n",
      " 6   case_closed           841704 non-null  object \n",
      " 7   dept_division         841704 non-null  object \n",
      " 8   service_request_type  841704 non-null  object \n",
      " 9   SLA_days              841671 non-null  float64\n",
      " 10  case_status           841704 non-null  object \n",
      " 11  source_id             841704 non-null  object \n",
      " 12  request_address       841704 non-null  object \n",
      " 13  council_district      841704 non-null  int64  \n",
      "dtypes: float64(2), int64(2), object(10)\n",
      "memory usage: 89.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# let's first import the case dataset as a pandas df\n",
    "\n",
    "df = pd.read_csv(\"/Users/mijailmariano/codeup-data-science/pyspark_exercises/data/case.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+----------------+------------+---------+-------------------+-----------+----------------+--------------------+-----------+-----------+---------+--------------------+----------------+\n",
      "|   case_id|case_opened_date|case_closed_date|SLA_due_date|case_late|      num_days_late|case_closed|   dept_division|service_request_type|   SLA_days|case_status|source_id|     request_address|council_district|\n",
      "+----------+----------------+----------------+------------+---------+-------------------+-----------+----------------+--------------------+-----------+-----------+---------+--------------------+----------------+\n",
      "|1014127332|     1/1/18 0:42|    1/1/18 12:29|9/26/20 0:42|       NO| -998.5087616000001|        YES|Field Operations|        Stray Animal|      999.0|     Closed| svcCRMLS|2315  EL PASO ST,...|               5|\n",
      "|1014127333|     1/1/18 0:46|     1/3/18 8:11| 1/5/18 8:30|       NO|-2.0126041669999997|        YES|     Storm Water|Removal Of Obstru...|4.322222222|     Closed| svcCRMSS|2215  GOLIAD RD, ...|               3|\n",
      "+----------+----------------+----------------+------------+---------+-------------------+-----------+----------------+--------------------+-----------+-----------+---------+--------------------+----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the case, department, and source data into their own spark dataframes.\n",
    "\n",
    "file_path = \"/Users/mijailmariano/codeup-data-science/pyspark_exercises/data/case.csv\"\n",
    "\n",
    "# specifiying the column schema/types (best when computational speed is a priority)\n",
    "# schema = StructType([StructField(\"case_closed_date\", StringType())])\n",
    "\n",
    "# case dataset\n",
    "case = spark.read.csv( \n",
    "    file_path,\n",
    "    sep = \",\",\n",
    "    header = True, \n",
    "    inferSchema = True\n",
    ")\n",
    "\n",
    "# check the csv loaded correctly\n",
    "case.show(2) # checks out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dept dataset\n",
    "dept = spark.read.csv( \n",
    "    \"/Users/mijailmariano/codeup-data-science/pyspark_exercises/data/dept.csv\",\n",
    "    sep = \",\",\n",
    "    header = True, \n",
    "    inferSchema = True\n",
    ")\n",
    "\n",
    "# source dataset\n",
    "source = spark.read.csv( \n",
    "    \"/Users/mijailmariano/codeup-data-science/pyspark_exercises/data/source.csv\",\n",
    "    sep = \",\",\n",
    "    header = True, \n",
    "    inferSchema = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Let's see how writing to the local disk works in spark:\n",
    "\n",
    "# Write the code necessary to store the source data in both csv and json format, store these as sources_csv and sources_json\n",
    "# Inspect your folder structure. What do you notice?\n",
    "\n",
    "case.write.csv(\"/Users/mijailmariano/codeup-data-science/pyspark_exercises/data/case_spark.csv\", mode = \"overwrite\")\n",
    "dept.write.json(\"/Users/mijailmariano/codeup-data-science/pyspark_exercises/data/dept_spark.csv\", mode = \"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('case_id', 'int'),\n",
       " ('case_opened_date', 'string'),\n",
       " ('case_closed_date', 'string'),\n",
       " ('case_due_date', 'string'),\n",
       " ('case_late', 'string'),\n",
       " ('num_days_late', 'double'),\n",
       " ('case_closed', 'string'),\n",
       " ('dept_division', 'string'),\n",
       " ('service_request_type', 'string'),\n",
       " ('SLA_days', 'double'),\n",
       " ('case_status', 'string'),\n",
       " ('source_id', 'string'),\n",
       " ('request_address', 'string'),\n",
       " ('council_district', 'int')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# column names and dtype in case dataset\n",
    "\n",
    "case.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+-------------+\n",
      "|case_opened_date|case_closed_date|case_due_date|\n",
      "+----------------+----------------+-------------+\n",
      "|     1/1/18 0:42|    1/1/18 12:29| 9/26/20 0:42|\n",
      "|     1/1/18 0:46|     1/3/18 8:11|  1/5/18 8:30|\n",
      "|     1/1/18 0:48|     1/2/18 7:57|  1/5/18 8:30|\n",
      "|     1/1/18 1:29|     1/2/18 8:13| 1/17/18 8:30|\n",
      "+----------------+----------------+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/14 02:01:38 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 3858400 ms exceeds timeout 120000 ms\n",
      "22/09/14 02:01:39 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "# what do the date columns look like?\n",
    "\n",
    "case.select(\"case_opened_date\", \"case_closed_date\", \"case_due_date\").show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+\n",
      "|case_closed|case_late|\n",
      "+-----------+---------+\n",
      "|       true|    false|\n",
      "|       true|    false|\n",
      "|       true|    false|\n",
      "|       true|    false|\n",
      "|       true|     true|\n",
      "|       true|    false|\n",
      "|       true|    false|\n",
      "|       true|    false|\n",
      "|       true|    false|\n",
      "|       true|    false|\n",
      "+-----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the data in your dataframes. Are the data types appropriate? Write the code necessary to cast the values to the appropriate types.\n",
    "\n",
    "case = case.withColumnRenamed(\"SLA_due_date\", \"case_due_date\")\n",
    "\n",
    "#  'casting' \"no\" and \"yes\" expressions to correct boolean data types\n",
    "# note that to cast, you will need to use the 'withColumn' method\n",
    "# mach expression such as 'Yes' and 'No' require additional \"\" (quotes) when combined with the pyspark 'expr' method\n",
    "case = case.withColumn('case_closed', expr('case_closed == \"YES\"')).withColumn('case_late', expr('case_late == \"YES\"'))\n",
    "\n",
    "case.select('case_closed', 'case_late').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
